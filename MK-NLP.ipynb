{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Swahili Sentiment Analysis\n",
    " Classifying Swahili tweets into positive, negative, and neutral sentiments.\n",
    " This solution fine-tunes a pre-trained RoBERTa (**WECHSEL-Swahili**) model using the transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, GlobalMaxPooling1D, GlobalAveragePooling1D, Concatenate\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from joblib import dump, load\n",
    "from transformers import TFRobertaModel\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, BatchNormalization\n",
    "\n",
    "# Custom loss function with label smoothing\n",
    "class SparseCategoricalCrossentropyWithLabelSmoothing(Loss):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, label_smoothing=0.1, name=\"sparse_categorical_crossentropy_with_label_smoothing\"):\n",
    "        super().__init__(name=name)\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    # Define the loss calculation\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Convert true labels to one-hot encoding\n",
    "        labels = tf.one_hot(tf.squeeze(tf.cast(y_true, tf.int32)), depth=y_pred.shape[-1])\n",
    "        # Apply categorical cross-entropy loss with label smoothing\n",
    "        return tf.keras.losses.categorical_crossentropy(labels, y_pred, label_smoothing=self.label_smoothing)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load the training data\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "# Encode sentiment labels using LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "data['Labels'] = encoder.fit_transform(data['Labels'])\n",
    "# Save the label encoder for later use\n",
    "dump(encoder, 'sw-encoder.joblib')\n",
    "\n",
    "# List of pre-trained model names\n",
    "models = ['benjamin/roberta-base-wechsel-swahili']\n",
    "\n",
    "# Split the data into training and holdout sets\n",
    "train_data, holdout_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "# Iterate over pre-trained models\n",
    "for i, model_name in enumerate(models):\n",
    "    \n",
    "    # Define early stopping and model checkpoint callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "    model_checkpoint = ModelCheckpoint(f'sw-best_model_{i}.h5', monitor='val_loss', save_best_only=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=1e-7, verbose=1)\n",
    "    \n",
    "    # Load the tokenizer and pre-trained model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = TFAutoModel.from_pretrained(model_name)\n",
    "    # Save the tokenizer for later use\n",
    "    tokenizer.save_pretrained(f'sw-tokenizer_{i}')\n",
    "    \n",
    "    # Tokenize input data using the pre-trained tokenizer\n",
    "    inputs = tokenizer(train_data['Tweets'].to_list(), return_tensors='tf', padding=True, truncation=True, max_length=512)\n",
    "    labels = train_data['Labels'].to_numpy()\n",
    "\n",
    "    # Convert tokenized input to NumPy arrays\n",
    "    input_ids = inputs['input_ids'].numpy()\n",
    "    attention_mask = inputs['attention_mask'].numpy()\n",
    "\n",
    "    # Perform Stratified K-Fold cross-validation\n",
    "    \n",
    "    # Initialize Stratified K-Fold cross-validator\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(input_ids, labels)):        \n",
    "                \n",
    "        train_input_ids, val_input_ids = input_ids[train_index], input_ids[val_index]\n",
    "        train_attention_mask, val_attention_mask = attention_mask[train_index], attention_mask[val_index]\n",
    "        train_labels, val_labels = labels[train_index], labels[val_index]\n",
    "\n",
    "        # Compute class weights and create dictionary\n",
    "        class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "        \n",
    "        # Define model architecture\n",
    "        \n",
    "        # Define input layers for model\n",
    "        input_ids_layer = Input(shape=(None,), dtype=tf.int32, name='input_ids')\n",
    "        attention_mask_layer = Input(shape=(None,), dtype=tf.int32, name='attention_mask')\n",
    "        \n",
    "        # Retrieve base output from pre-trained model\n",
    "        base_output = model([input_ids_layer, attention_mask_layer])[0]\n",
    "\n",
    "        # Adding dropout to the embeddings\n",
    "        base_output = Dropout(0.2)(base_output)\n",
    "\n",
    "        # Adding Bidirectional LSTM Layer\n",
    "        base_output = Bidirectional(LSTM(64, return_sequences=True))(base_output)\n",
    "\n",
    "        # Batch normalization for regularizing and speeding up training\n",
    "        base_output = BatchNormalization()(base_output)\n",
    "\n",
    "       # Pooling layers to capture different aspects of the data\n",
    "        max_pool_output = GlobalMaxPooling1D()(base_output)  # Global Max Pooling to capture strongest features\n",
    "        avg_pool_output = GlobalAveragePooling1D()(base_output)  # Global Average Pooling to capture overall features\n",
    "\n",
    "        # Concatenate the outputs from max pooling and average pooling\n",
    "        pooled_output = Concatenate()([max_pool_output, avg_pool_output])\n",
    "\n",
    "        # Fully connected layer for classification with softmax activation and L2 regularization \n",
    "        output = Dense(3, activation='softmax', kernel_regularizer=regularizers.l2(0.02))(pooled_output)\n",
    "\n",
    "        # Create a new model using input layers and the constructed output layer\n",
    "        new_model = Model(inputs=[input_ids_layer, attention_mask_layer], outputs=output)\n",
    "\n",
    "        # Compile the model with custom loss function\n",
    "        optimizer = Adam(learning_rate=1e-5)\n",
    "        loss = SparseCategoricalCrossentropyWithLabelSmoothing()\n",
    "        metrics = ['accuracy']\n",
    "        new_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "        # Train the model with model checkpoints, early stopping, and learning rate reduction for training optimization\n",
    "        history = new_model.fit({'input_ids': train_input_ids, 'attention_mask': train_attention_mask},\n",
    "                                train_labels, epochs=20,\n",
    "                                validation_data=({'input_ids': val_input_ids, 'attention_mask': val_attention_mask}, val_labels),\n",
    "                                class_weight=class_weights_dict,\n",
    "                                callbacks=[early_stopping, model_checkpoint, reduce_lr])\n",
    "\n",
    "        # Load the best model weights\n",
    "        new_model.load_weights(f'sw-best_model_{i}.h5')\n",
    "\n",
    "        # Save trained fold model\n",
    "        new_model.save(f'sw-model_{i}_fold_{fold}.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model's performance on the holdout data by loading the trained model, tokenizing the holdout data, making predictions, and then printing the confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom loss function with label smoothing with reduction set to AUTO\n",
    "class SparseCategoricalCrossentropyWithLabelSmoothing(tf.keras.losses.Loss):\n",
    "    def __init__(self, label_smoothing=0.1, reduction=tf.keras.losses.Reduction.AUTO, name=\"sparse_categorical_crossentropy_with_label_smoothing\"):\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Convert true labels to one-hot encoding\n",
    "        labels = tf.one_hot(tf.squeeze(tf.cast(y_true, tf.int32)), depth=y_pred.shape[-1])\n",
    "        # Apply categorical cross-entropy loss with label smoothing\n",
    "        return tf.keras.losses.categorical_crossentropy(labels, y_pred, label_smoothing=self.label_smoothing)\n",
    "\n",
    "# Define the custom_objects dictionary for model loading\n",
    "custom_objects = {\n",
    "    \"SparseCategoricalCrossentropyWithLabelSmoothing\": SparseCategoricalCrossentropyWithLabelSmoothing,\n",
    "    'TFRobertaModel': TFRobertaModel  # TFRobertaModel is a required custom object\n",
    "}\n",
    "\n",
    "# Load the last saved model using custom_objects\n",
    "model = tf.keras.models.load_model(\n",
    "    f'sw-model_0_fold_9.keras',\n",
    "    custom_objects=custom_objects\n",
    ")\n",
    "\n",
    "# Load the tokenizer used during training\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'sw-tokenizer_0')\n",
    "\n",
    "# Tokenize and predict on holdout data\n",
    "inputs = tokenizer(holdout_data['Tweets'].to_list(), return_tensors='tf', padding=True, truncation=True, max_length=512)\n",
    "input_ids = inputs['input_ids'].numpy()\n",
    "attention_mask = inputs['attention_mask'].numpy()\n",
    "predictions = model.predict({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
    "\n",
    "# Convert output probabilities to class labels\n",
    "predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(holdout_data['Labels'], predictions))\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(holdout_data['Labels'], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict labels for the test data, transform the predictions back to their original labels, create and save a CSV file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom loss function with label smoothing with reduction set to AUTO\n",
    "class SparseCategoricalCrossentropyWithLabelSmoothing(tf.keras.losses.Loss):\n",
    "    def __init__(self, label_smoothing=0.1, reduction=tf.keras.losses.Reduction.AUTO, name=\"sparse_categorical_crossentropy_with_label_smoothing\"):\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Convert true labels to one-hot encoding\n",
    "        labels = tf.one_hot(tf.squeeze(tf.cast(y_true, tf.int32)), depth=y_pred.shape[-1])\n",
    "        # Apply categorical cross-entropy loss with label smoothing\n",
    "        return tf.keras.losses.categorical_crossentropy(labels, y_pred, label_smoothing=self.label_smoothing)\n",
    "\n",
    "# Define the custom_objects dictionary for model loading\n",
    "custom_objects = {\n",
    "    \"SparseCategoricalCrossentropyWithLabelSmoothing\": SparseCategoricalCrossentropyWithLabelSmoothing,\n",
    "    'TFRobertaModel': TFRobertaModel  # TFRobertaModel is a required custom object\n",
    "}\n",
    "\n",
    "# Load the last saved model using custom_objects\n",
    "model = tf.keras.models.load_model(\n",
    "    f'sw-model_0_fold_9.keras',\n",
    "    custom_objects=custom_objects\n",
    ")\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Load the tokenizer used during training\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'sw-tokenizer_0')\n",
    "\n",
    "# Tokenize the test data\n",
    "inputs = tokenizer(test_data['Tweets'].to_list(), return_tensors='tf', padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Make predictions on the test data\n",
    "output = model.predict({'input_ids': inputs['input_ids'].numpy(), 'attention_mask': inputs['attention_mask'].numpy()})\n",
    "\n",
    "# Take argmax to get predicted class\n",
    "test_predictions = np.argmax(output, axis=-1)\n",
    "\n",
    "# Load the encoder used during training\n",
    "encoder = load('sw-encoder.joblib')\n",
    "\n",
    "# Reverse-transform the predicted labels\n",
    "test_predictions = encoder.inverse_transform(test_predictions)\n",
    "\n",
    "# Prepare the submission DataFrame\n",
    "submission = pd.DataFrame()\n",
    "submission['ID'] = test_data['ID']  # Make sure the column names match\n",
    "submission['Labels'] = test_predictions\n",
    "\n",
    "# Save the submission as a CSV file\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
